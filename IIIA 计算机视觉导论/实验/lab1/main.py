# --- 0. å¯¼å…¥å¿…è¦çš„åº“ ---
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import matplotlib
# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
import os
from datetime import datetime

# ==============================================================================
# --- 1. å®éªŒé…ç½®åŒº ---
#åœ¨è¿™é‡Œé›†ä¸­ä¿®æ”¹æ‰€æœ‰å®éªŒå‚æ•°ï¼Œæ–¹ä¾¿è¿›è¡Œå¤šç»„å¯¹æ¯”å®éªŒ
# ==============================================================================
CONFIG = {
    # --- æ•°æ®ä¸æ¨¡å‹é…ç½® ---
    "file_path": "dataset.csv",
    # æœ€ä½³æ¶æ„ï¼šæµ…å±‚ç½‘ç»œ [32] - æµ‹è¯•å‡†ç¡®ç‡99.75% (æœ€é«˜)
    "hidden_layers": [32],
    # æœ€ä½³æ¿€æ´»å‡½æ•°ï¼šReLU
    "activation_function": nn.ReLU,
    
    # --- è®­ç»ƒå‚æ•° ---
    "train_ratio": 0.9,
    # æ ‡å‡†æ‰¹æ¬¡å¤§å°ï¼š32 - æ¥è‡ªæœ€ä½³å®éªŒç»“æœ
    "batch_size": 32,
    # æ ‡å‡†å­¦ä¹ ç‡ï¼š0.001 - æ¥è‡ªæœ€ä½³å®éªŒç»“æœ
    "learning_rate": 0.001,
    # æ ‡å‡†è®­ç»ƒè½®æ•°ï¼š15 - æ¥è‡ªæœ€ä½³å®éªŒç»“æœ
    "epochs": 15,
    
    # --- æ­£åˆ™åŒ–å‚æ•° ---
    # æ— Dropoutï¼š0.0 - æ¥è‡ªæœ€ä½³å®éªŒç»“æœ
    "dropout_rate": 0.0,
    
    # --- ç»“æœè®°å½• ---
    "log_file": "experiment_log.csv"
}


# --- 2. åŠ¨æ€ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰ ---
# è¿™ä¸ªç±»ç°åœ¨å¯ä»¥æ ¹æ®CONFIGåŠ¨æ€ç”Ÿæˆä¸åŒç»“æ„çš„ç½‘ç»œ
class DynamicFNN(nn.Module):
    """ä¸€ä¸ªå¯ä»¥æ ¹æ®é…ç½®åŠ¨æ€æ„å»ºçš„å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ”¯æŒDropout"""
    def __init__(self, input_size, hidden_layers_config, num_classes, activation_fn, dropout_rate=0.0):
        super(DynamicFNN, self).__init__()
        
        # åˆ›å»ºä¸€ä¸ªç”¨äºå­˜æ”¾ç½‘ç»œå±‚çš„åˆ—è¡¨
        layers = []
        current_input_size = input_size
        
        # éå†é…ç½®ä¸­çš„éšè—å±‚ï¼Œå¹¶é€å±‚æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        for i, layer_size in enumerate(hidden_layers_config):
            # æ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚ (Linear)
            layers.append(nn.Linear(current_input_size, layer_size))
            # æ·»åŠ æŒ‡å®šçš„æ¿€æ´»å‡½æ•°
            layers.append(activation_fn())
            
            # æ·»åŠ Dropoutï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
            if dropout_rate > 0 and i < len(hidden_layers_config) - 1:
                layers.append(nn.Dropout(dropout_rate))
            
            # æ›´æ–°ä¸‹ä¸€å±‚çš„è¾“å…¥å°ºå¯¸
            current_input_size = layer_size
            
        # æ·»åŠ æœ€åçš„è¾“å‡ºå±‚
        layers.append(nn.Linear(current_input_size, num_classes))
        
        # ä½¿ç”¨ nn.Sequential å°†æ‰€æœ‰å±‚ç»„åˆæˆä¸€ä¸ªç½‘ç»œæ¨¡å‹
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        """å®šä¹‰æ•°æ®åœ¨ç½‘ç»œä¸­çš„å‰å‘ä¼ æ’­è·¯å¾„"""
        return self.network(x)

# --- 3. è¾…åŠ©å‡½æ•° ---
def evaluate_model(model, loader, criterion, device):
    """
    åœ¨ç»™å®šçš„æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡ã€‚
    
    Args:
        model (nn.Module): è¦è¯„ä¼°çš„æ¨¡å‹ã€‚
        loader (DataLoader): åŒ…å«è¯„ä¼°æ•°æ®çš„æ•°æ®åŠ è½½å™¨ã€‚
        criterion: æŸå¤±å‡½æ•°ã€‚
        device: è®¡ç®—è®¾å¤‡ (cpu æˆ– cuda)ã€‚

    Returns:
        tuple: (å¹³å‡æŸå¤±, å‡†ç¡®ç‡)ã€‚
    """
    model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆè¿™ä¼šå…³é—­dropoutç­‰å±‚ï¼‰
    total_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():  # åœ¨è¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼Œå¯ä»¥èŠ‚çœè®¡ç®—èµ„æº
        for data, targets in loader:
            data, targets = data.to(device), targets.to(device)
            # å‰å‘ä¼ æ’­è·å–æ¨¡å‹è¾“å‡º
            outputs = model(data)
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, targets)
            # ç´¯åŠ æ€»æŸå¤±ï¼ˆä¹˜ä»¥æ‰¹é‡å¤§å°ä»¥å¾—åˆ°æ‰¹æ¬¡æ€»æŸå¤±ï¼‰
            total_loss += loss.item() * len(targets)
            # æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„ç±»ä½œä¸ºé¢„æµ‹ç»“æœ
            _, predicted = torch.max(outputs.data, 1)
            # ç´¯åŠ æ ·æœ¬æ€»æ•°
            total += targets.size(0)
            # ç´¯åŠ é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ•°
            correct += (predicted == targets).sum().item()
    
    # è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    avg_loss = total_loss / total
    accuracy = 100 * correct / total
    return avg_loss, accuracy

def log_experiment(config, results, log_file):
    """
    å°†å®éªŒé…ç½®å’Œç»“æœè®°å½•åˆ°CSVæ–‡ä»¶ä¸­ã€‚

    Args:
        config (dict): åŒ…å«å®éªŒè¶…å‚æ•°çš„å­—å…¸ã€‚
        results (dict): åŒ…å«å®éªŒæœ€ç»ˆæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸ã€‚
        log_file (str): æ—¥å¿—æ–‡ä»¶åã€‚
    """
    # å°†ç½‘ç»œç»“æ„å’Œæ¿€æ´»å‡½æ•°è½¬æ¢ä¸ºæ˜“äºé˜…è¯»çš„å­—ç¬¦ä¸²æ ¼å¼
    architecture = f"Input(2)-{'-'.join(map(str, config['hidden_layers']))}-Output(4)"
    activation = config['activation_function'].__name__
    
    # å‡†å¤‡è¦è®°å½•çš„ä¸€è¡Œæ•°æ®
    log_data = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'architecture': architecture,
        'activation': activation,
        'learning_rate': config['learning_rate'],
        'batch_size': config['batch_size'],
        'epochs': config['epochs'],
        'final_train_loss': results['train_loss'],
        'final_train_acc': results['train_acc'],
        'final_test_loss': results['test_loss'],
        'final_test_acc': results['test_acc']
    }
    
    log_df = pd.DataFrame([log_data])
    
    # å¦‚æœæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
    if not os.path.exists(log_file):
        log_df.to_csv(log_file, index=False)
    # å¦åˆ™ï¼Œè¿½åŠ å†™å…¥ï¼Œä¸åŒ…å«è¡¨å¤´
    else:
        log_df.to_csv(log_file, mode='a', header=False, index=False)
    print(f"\nå®éªŒç»“æœå·²è®°å½•åˆ° {log_file}")


# --- 4. ä¸»æ‰§è¡Œæµç¨‹ ---
if __name__ == '__main__':
    # --- æ•°æ®å‡†å¤‡ ---
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # ä½¿ç”¨pandasåŠ è½½CSVï¼Œå‡è®¾æ²¡æœ‰è¡¨å¤´
    df = pd.read_csv(CONFIG['file_path'], header=None)
    features = df.iloc[:, :-1].values
    labels = df.iloc[:, -1].values
    # å°†æ ‡ç­¾ä»1-4è½¬æ¢ä¸º0-3
    labels = labels - 1

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=1 - CONFIG['train_ratio'], random_state=42, shuffle=True
    )

    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # è½¬æ¢ä¸ºPyTorch Tensorså¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long)

    # åˆ›å»ºDatasetå’ŒDataLoader
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    
    train_loader = DataLoader(dataset=train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)
    # ç”¨äºå…¨é‡è¯„ä¼°çš„åŠ è½½å™¨
    full_train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset))
    test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))

    # --- æ¨¡å‹åˆå§‹åŒ– ---
    input_dim = X_train.shape[1]
    output_dim = len(np.unique(labels))
    
    # æ ¹æ®CONFIGå®ä¾‹åŒ–åŠ¨æ€æ¨¡å‹
    model = DynamicFNN(
        input_size=input_dim, 
        hidden_layers_config=CONFIG['hidden_layers'],
        num_classes=output_dim,
        activation_fn=CONFIG['activation_function'],
        dropout_rate=CONFIG.get('dropout_rate', 0.0)
    ).to(device)
    
    print("\n--- ç¥ç»ç½‘ç»œæ¶æ„ ---")
    print(model)

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])

    # --- è®­ç»ƒä¸è¯„ä¼° ---
    # åˆ›å»ºåˆ—è¡¨ç”¨äºå­˜å‚¨å†å²æ•°æ®ä»¥ä¾›åç»­å¯è§†åŒ–
    history = {
        'steps': [],
        'train_loss': [], 'test_loss': [],
        'train_acc': [], 'test_acc': []
    }
    
    print("\n--- å¼€å§‹è®­ç»ƒ ---")
    global_step = 0
    for epoch in range(CONFIG['epochs']):
        model.train() # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        epoch_train_loss = 0
        epoch_train_acc = 0
        epoch_test_loss = 0
        epoch_test_acc = 0
        batch_count = 0
        
        for i, (batch_features, batch_labels) in enumerate(train_loader):
            batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_features)
            loss = criterion(outputs, batch_labels)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # === åœ¨æ¯ä¸ªmini-batchè®­ç»ƒåï¼Œåœ¨å®Œæ•´çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼° ===
            train_loss, train_acc = evaluate_model(model, full_train_loader, criterion, device)
            test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)
            
            # è®°å½•æ•°æ®ç”¨äºç»˜åˆ¶minibatchæ›²çº¿
            history['steps'].append(global_step)
            history['train_loss'].append(train_loss)
            history['test_loss'].append(test_loss)
            history['train_acc'].append(train_acc)
            history['test_acc'].append(test_acc)
            
            # ç´¯åŠ ç”¨äºè®¡ç®—epochå¹³å‡å€¼
            epoch_train_loss += train_loss
            epoch_train_acc += train_acc
            epoch_test_loss += test_loss
            epoch_test_acc += test_acc
            batch_count += 1
            
            global_step += 1
        
        # è®¡ç®—epochå¹³å‡å€¼å¹¶è¾“å‡º
        avg_train_loss = epoch_train_loss / batch_count
        avg_train_acc = epoch_train_acc / batch_count
        avg_test_loss = epoch_test_loss / batch_count
        avg_test_acc = epoch_test_acc / batch_count
        
        print(f'Epoch [{epoch+1:2d}/{CONFIG["epochs"]}] | '
              f'Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.2f}% | '
              f'Test Loss: {avg_test_loss:.4f} | Test Acc: {avg_test_acc:.2f}%')

    print("--- è®­ç»ƒå®Œæˆ ---\n")

    # --- æœ€ç»ˆè¯„ä¼°ä¸è®°å½• ---
    final_train_loss, final_train_acc = evaluate_model(model, full_train_loader, criterion, device)
    final_test_loss, final_test_acc = evaluate_model(model, test_loader, criterion, device)

    print("--- æœ€ç»ˆè¯„ä¼°ç»“æœ ---")
    print(f'æœ€ç»ˆè®­ç»ƒé›† -> æŸå¤±: {final_train_loss:.4f}, å‡†ç¡®ç‡: {final_train_acc:.2f}%')
    print(f'æœ€ç»ˆæµ‹è¯•é›† -> æŸå¤±: {final_test_loss:.4f}, å‡†ç¡®ç‡: {final_test_acc:.2f}%')

    # å°†æœ¬æ¬¡å®éªŒç»“æœè®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
    final_results = {
        'train_loss': final_train_loss, 'train_acc': final_train_acc,
        'test_loss': final_test_loss, 'test_acc': final_test_acc
    }
    log_experiment(CONFIG, final_results, CONFIG['log_file'])

    # --- 5. ç»“æœå¯è§†åŒ– ---
    print("\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # ç»˜åˆ¶æŸå¤±æ›²çº¿ (Minibatchçº§åˆ«)
    ax1.plot(history['steps'], history['train_loss'], label='Train Loss', alpha=0.7, linewidth=1)
    ax1.plot(history['steps'], history['test_loss'], label='Test Loss', alpha=0.7, linewidth=1)
    ax1.set_title('Loss vs Mini-batch Steps\n(Best Config: [32] layers, LR=0.001, Dropout=0.0)')
    ax1.set_xlabel('Mini-batch Step')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(bottom=0)  # æŸå¤±ä»0å¼€å§‹æ˜¾ç¤º

    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿ (Minibatchçº§åˆ«)
    ax2.plot(history['steps'], history['train_acc'], label='Train Accuracy', alpha=0.7, linewidth=1)
    ax2.plot(history['steps'], history['test_acc'], label='Test Accuracy', alpha=0.7, linewidth=1)
    ax2.set_title('Accuracy vs Mini-batch Steps\n(Final: Train={:.1f}%, Test={:.1f}%)'.format(
        final_train_acc, final_test_acc))
    ax2.set_xlabel('Mini-batch Step')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(95, 100)  # å‡†ç¡®ç‡ä»95%å¼€å§‹æ˜¾ç¤ºï¼Œæ›´å¥½åœ°å±•ç¤ºå˜åŒ–

    plt.tight_layout()
    plt.show()
    
    # è¾“å‡ºè®­ç»ƒæ€»ç»“
    print(f"\nğŸ“Š è®­ç»ƒæ€»ç»“:")
    print(f"   æ¶æ„: Input(2) â†’ {CONFIG['hidden_layers']} â†’ Output(4)")
    print(f"   ä¼˜åŒ–å™¨: Adam (lr={CONFIG['learning_rate']})")
    print(f"   æ­£åˆ™åŒ–: Dropout({CONFIG.get('dropout_rate', 0.0)})")
    print(f"   æ‰¹æ¬¡å¤§å°: {CONFIG['batch_size']}")
    print(f"   è®­ç»ƒè½®æ•°: {CONFIG['epochs']}")
    print(f"   æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_test_acc:.2f}%")
    print(f"   æ€»è®­ç»ƒæ­¥æ•°: {len(history['steps'])}")
